{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96fbac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import os\n",
    "import ntpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc9d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e326050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Importing faces...\n",
      "[INFO] Faces well imported\n",
      "[INFO] Starting Webcam...\n",
      "[INFO] Webcam well started\n",
      "[INFO] Detecting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x770fa7e8e970>, array([[[188, 181, 181],\n        [189, 182, 182],\n        [189, 181, 184],\n        ...,\n        [166, 167, 162],\n        [165, 166, 161],\n        [165, 166, 161]],\n\n       [[189, 182, 182],\n        [189, 182, 182],\n        [187, 183, 182],\n        ...,\n        [166, 167, 162],\n        [166, 167, 162],\n        [166, 167, 162]],\n\n       [[187, 183, 180],\n        [186, 182, 179],\n        [185, 181, 178],\n        ...,\n        [165, 166, 161],\n        [166, 168, 160],\n        [166, 168, 160]],\n\n       ...,\n\n       [[ 70,  82, 110],\n        [ 69,  81, 109],\n        [ 69,  80, 111],\n        ...,\n        [108, 138, 178],\n        [116, 155, 192],\n        [109, 149, 185]],\n\n       [[ 71,  83, 111],\n        [ 69,  81, 109],\n        [ 69,  80, 111],\n        ...,\n        [107, 137, 177],\n        [107, 144, 182],\n        [106, 143, 181]],\n\n       [[ 70,  82, 110],\n        [ 69,  81, 109],\n        [ 70,  81, 114],\n        ...,\n        [100, 132, 173],\n        [107, 144, 184],\n        [101, 138, 178]]], shape=(480, 640, 3), dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x770fa7e76030>; kwargs: num_jitters=1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 170\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[ERROR] Cannot grab frame from webcam\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43measy_face_reco\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_face_encodings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown_face_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# Affiche la frame annotée\u001b[39;00m\n\u001b[32m    173\u001b[39m cv2.imshow(\u001b[33m'\u001b[39m\u001b[33mEasy Facial Recognition App\u001b[39m\u001b[33m'\u001b[39m, frame)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36measy_face_reco\u001b[39m\u001b[34m(frame, known_face_encodings, known_face_names)\u001b[39m\n\u001b[32m     80\u001b[39m rgb_small_frame = frame[:, :, ::-\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# Conversion BGR (cv2) -> RGB (dlib)\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Encodage des visages détectés dans la frame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m face_encodings_list, face_locations_list, landmarks_list = \u001b[43mencode_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_small_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m face_names = []\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m face_encoding \u001b[38;5;129;01min\u001b[39;00m face_encodings_list:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mencode_face\u001b[39m\u001b[34m(image)\u001b[39m\n\u001b[32m     54\u001b[39m shape = pose_predictor_68_point(image, face_location)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Calcul encoding du visage (vecteur de caractéristiques)\u001b[39;00m\n\u001b[32m     57\u001b[39m face_encodings_list.append(\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     np.array(\u001b[43mface_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_face_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_jitters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     59\u001b[39m )\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Conversion shape en tableau numpy pour landmarks\u001b[39;00m\n\u001b[32m     61\u001b[39m shape_np = face_utils.shape_to_np(shape)\n",
      "\u001b[31mTypeError\u001b[39m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x770fa7e8e970>, array([[[188, 181, 181],\n        [189, 182, 182],\n        [189, 181, 184],\n        ...,\n        [166, 167, 162],\n        [165, 166, 161],\n        [165, 166, 161]],\n\n       [[189, 182, 182],\n        [189, 182, 182],\n        [187, 183, 182],\n        ...,\n        [166, 167, 162],\n        [166, 167, 162],\n        [166, 167, 162]],\n\n       [[187, 183, 180],\n        [186, 182, 179],\n        [185, 181, 178],\n        ...,\n        [165, 166, 161],\n        [166, 168, 160],\n        [166, 168, 160]],\n\n       ...,\n\n       [[ 70,  82, 110],\n        [ 69,  81, 109],\n        [ 69,  80, 111],\n        ...,\n        [108, 138, 178],\n        [116, 155, 192],\n        [109, 149, 185]],\n\n       [[ 71,  83, 111],\n        [ 69,  81, 109],\n        [ 69,  80, 111],\n        ...,\n        [107, 137, 177],\n        [107, 144, 182],\n        [106, 143, 181]],\n\n       [[ 70,  82, 110],\n        [ 69,  81, 109],\n        [ 70,  81, 114],\n        ...,\n        [100, 132, 173],\n        [107, 144, 184],\n        [101, 138, 178]]], shape=(480, 640, 3), dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x770fa7e76030>; kwargs: num_jitters=1"
     ]
    }
   ],
   "source": [
    "# Chargement des modèles pré-entraînés \n",
    "# Remplacez les chemins absolus vers vos fichiers .dat ici\n",
    "pose_predictor_68_point = dlib.shape_predictor(\"/home/la-mus/AI/projet/pretrained_model/shape_predictor_68_face_landmarks.dat\")\n",
    "pose_predictor_5_point = dlib.shape_predictor(\"/home/la-mus/AI/projet/pretrained_model/shape_predictor_5_face_landmarks.dat\")\n",
    "face_encoder = dlib.face_recognition_model_v1(\"/home/la-mus/AI/projet/pretrained_model/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "# Détecteur de visages frontal\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Fonction de transformation des coordonnées \n",
    "def transform(image, face_locations):\n",
    "    \"\"\"\n",
    "    Convertit les rectangles de détection de visage dlib (top, right, bottom, left) \n",
    "    en coordonnées gérées pour OpenCV et les limites de l'image.\n",
    "    \n",
    "    Args :\n",
    "        image (numpy array) : Image source.\n",
    "        face_locations (list) : Liste d'objets rectangles dlib délimitant les visages.\n",
    "\n",
    "    Retour :\n",
    "        coord_faces (list) : Liste de tuples coordonnées corrigées (top, right, bottom, left).\n",
    "    \"\"\"\n",
    "    coord_faces = []\n",
    "    for face in face_locations:\n",
    "        rect = face.top(), face.right(), face.bottom(), face.left()\n",
    "        coord_face = (\n",
    "            max(rect[0], 0),                    # top limité à 0 minimum\n",
    "            min(rect[1], image.shape[1]),      # right limité aux bords de l'image\n",
    "            min(rect[2], image.shape[0]),      # bottom limité aux bords\n",
    "            max(rect[3], 0)                    # left limité à 0 minimum\n",
    "        )\n",
    "        coord_faces.append(coord_face)\n",
    "    return coord_faces\n",
    "\n",
    "# Fonction d'encodage des visages \n",
    "def encode_face(image):\n",
    "    \"\"\"\n",
    "    Détecte les visages dans l'image, calcule leurs encodages et points de repère (landmarks).\n",
    "    \n",
    "    Args :\n",
    "        image (numpy array) : Image RGB.\n",
    "\n",
    "    Retour :\n",
    "        face_encodings_list (list) : Liste des encodages des visages détectés.\n",
    "        face_locations (list) : Coordonnées des visages détectés.\n",
    "        landmarks_list (list) : Liste des points de repère pour chaque visage.\n",
    "    \"\"\"\n",
    "    face_locations = face_detector(image, 1)  # Détection visage avec dlib\n",
    "\n",
    "    face_encodings_list = []\n",
    "    landmarks_list = []\n",
    "\n",
    "    for face_location in face_locations:\n",
    "        # Extraction des points de repère (shape/landmarks)\n",
    "        shape = pose_predictor_68_point(image, face_location)\n",
    "\n",
    "        # Calcul encoding du visage (vecteur de caractéristiques)\n",
    "        face_encodings_list.append(\n",
    "            np.array(face_encoder.compute_face_descriptor(image, shape, num_jitters=1))\n",
    "        )\n",
    "        # Conversion shape en tableau numpy pour landmarks\n",
    "        shape_np = face_utils.shape_to_np(shape)\n",
    "        landmarks_list.append(shape_np)\n",
    "\n",
    "    # Transformation des coordonnées pour OpenCV\n",
    "    face_locations = transform(image, face_locations)\n",
    "\n",
    "    return face_encodings_list, face_locations, landmarks_list\n",
    "\n",
    "# Fonction principale de reconnaissance \n",
    "def easy_face_reco(frame, known_face_encodings, known_face_names):\n",
    "    \"\"\"\n",
    "    Analyse une image/frame vidéo, détecte et reconnaît les visages connus.\n",
    "    Dessine les boîtes et noms autour des visages détectés.\n",
    "\n",
    "    Args :\n",
    "        frame (numpy array) : Image BGR (vidéo frame).\n",
    "        known_face_encodings (list) : Liste des encodages des visages connus.\n",
    "        known_face_names (list) : Liste des noms associés aux visages connus.\n",
    "    \"\"\"\n",
    "    rgb_small_frame = frame[:, :, ::-1]  # Conversion BGR (cv2) -> RGB (dlib)\n",
    "\n",
    "    # Encodage des visages détectés dans la frame\n",
    "    face_encodings_list, face_locations_list, landmarks_list = encode_face(rgb_small_frame)\n",
    "\n",
    "    face_names = []\n",
    "\n",
    "    for face_encoding in face_encodings_list:\n",
    "        if len(face_encoding) == 0:\n",
    "            return np.empty((0))\n",
    "\n",
    "        # Calcul distance entre visages détectés et connus\n",
    "        vectors = np.linalg.norm(known_face_encodings - face_encoding, axis=1)\n",
    "\n",
    "        tolerance = 0.6\n",
    "\n",
    "        result = [vector <= tolerance for vector in vectors]\n",
    "\n",
    "        if True in result:\n",
    "            first_match_index = result.index(True)\n",
    "            name = known_face_names[first_match_index]\n",
    "        else:\n",
    "            name = \"Unknown\"\n",
    "        face_names.append(name)\n",
    "\n",
    "    # Dessin des rectangles et noms autour des visages détectés\n",
    "    for (top, right, bottom, left), name in zip(face_locations_list, face_names):\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame, (left, bottom - 30), (right, bottom), (0, 255, 0), cv2.FILLED)\n",
    "        cv2.putText(frame, name, (left + 2, bottom - 2), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1)\n",
    "\n",
    "    # Dessin des points de repère (landmarks) sur les visages\n",
    "    for shape in landmarks_list:\n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(frame, (x, y), 1, (255, 0, 255), -1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# ------------ Classe pour simuler les arguments dans notebook --\n",
    "# ---------------------------------------------------------------\n",
    "class Args:\n",
    "    # Chemin vers dossier contenant les images des visages connus\n",
    "    input = \"/home/la-mus/AI/projet/known_face\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# --------------------- Code principal --------------------------\n",
    "# ---------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = Args()\n",
    "\n",
    "    print('[INFO] Importing faces...')\n",
    "\n",
    "    # Recherche tous les fichiers images dans le dossier spécifié (jpg et png)\n",
    "    face_to_encode_path = Path(args.input)\n",
    "    files = [file_ for file_ in face_to_encode_path.rglob('*.jpg')]\n",
    "\n",
    "    for file_ in face_to_encode_path.rglob('*.png'):\n",
    "        files.append(file_)\n",
    "\n",
    "    if len(files) == 0:\n",
    "        raise ValueError(f'No faces detected in the directory: {face_to_encode_path}')\n",
    "\n",
    "    # Extraction des noms à partir des noms de fichiers (sans extension)\n",
    "    known_face_names = [os.path.splitext(ntpath.basename(file_))[0] for file_ in files]\n",
    "\n",
    "    # Encodage de chaque visage connu dans une liste\n",
    "    known_face_encodings = []\n",
    "    for file_ in files:\n",
    "        image = PIL.Image.open(file_)\n",
    "        image = np.array(image)\n",
    "        face_encoded = encode_face(image)[0][0]  # On prend le 1er visage détecté par image\n",
    "        known_face_encodings.append(face_encoded)\n",
    "\n",
    "    print('[INFO] Faces well imported')\n",
    "    print('[INFO] Starting Webcam...')\n",
    "\n",
    "    # Ouverture de la webcam\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    print('[INFO] Webcam well started')\n",
    "    print('[INFO] Detecting...')\n",
    "\n",
    "    # Boucle principale : lecture et traitement des frames webcam\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            print(\"[ERROR] Cannot grab frame from webcam\")\n",
    "            break\n",
    "\n",
    "        easy_face_reco(frame, np.array(known_face_encodings), known_face_names)\n",
    "\n",
    "        # Affiche la frame annotée\n",
    "        cv2.imshow('Easy Facial Recognition App', frame)\n",
    "\n",
    "        # Quitte la boucle si on appuie sur la touche 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    print('[INFO] Stopping System')\n",
    "\n",
    "    # Libération de la webcam et destruction des fenêtres\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d174e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d884b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (AI)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
